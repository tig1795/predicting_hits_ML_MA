{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kapitel 4 - Naive Bayes mit Multiclass Klassifizierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Kapitelübersicht <a class=\"anchor\" id=\"4-1\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Kapitel betrachten wird die <b>Multiclass Klassifizierung</b>, mit der wir im Gegensatz zu <b>binären Klassifizierung</b> Texte mit mehr als zwei Klassen klassifizieren können. Neben dem bereits bekannten <b>CountVectorizer</b> schauen wir uns noch eine Variation des Bag-of-Words Modells an, den <b>TfidfVectorizer</b>, der Wörter in eine <b>Tf-idf-Gewichtung</b> transformiert. Beide Vectorizer wenden wir sowohl auf den Filmrezensionen Datensatz als auch auf das Wikipedia-Korpus an. Beide Datensätze hatten wir bereits in Kapitel 2 näher betrachtet.\n",
    "\n",
    "<b>Abschnittsübersicht</b><br>\n",
    "\n",
    "[4.1. Kapitelübersicht](#4-1)<br>\n",
    "[4.2. Multiclass Klassifizierung mit dem Bag-of-Words Modell](#4-2)<br>\n",
    "[4.3. Der Tfidf-Vectorizer](#4-3)<br>\n",
    "[4.3.1 Anwendung auf die Film-Rezensionen](#4-3-1)<br>\n",
    "[4.3.2 Anwendung auf das Wikipedia-Korpus](#4-3-2)<br>\n",
    "\n",
    "Am Ende dieses Kapitel werden wir folgende Themen behandelt und/oder vertieft haben:\n",
    "- Multiclass Klassifizierung\n",
    "- Tfidf Vectorizer\n",
    "- Vergleich von Vectorizern\n",
    "- Anwendung des Multinomial Naive Bayes auf das Wikipedia-Korpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Multiclass Klassifizierung mit dem Bag-of-Words Modell <a class=\"anchor\" id=\"4-2\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bis jetzt hatten wir uns nur eine <b>binäre Klassifizierung</b> (englisch: binary classification) angeguckt. Dies bedeutet, dass die Anzahl der Klassen <i>binär</i> ist: Ein Text gehört entweder zu einer Klasse oder zu einer anderen Klassen. In unserem Fall waren die binären Klassen \"Sport\" und \"Kein Sport\". Diese Art der Klassifizierung wird z.B. für die Erkennung von Spam Mails verwendet, bei der Mails in die Kategorien \"Spam\" und \"Kein Spam\" unterteilt werden. In der Praxis stoßen wir jedoch öfters auf die Problematik, dass wir 3 oder mehr Klassen vorliegen haben und diese klassifizieren möchten. Dieses Klassifizierungsproblem nennt sich <b>multiclass classification</b> (deutsch: Mehrklassen-Klassifizierung) oder auch <b>multinomial classification</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bevor wir für die Multiclass Klassifikation das Wikipedia-Korpus benutzen, schauen wir uns das Einstiegsbeispiel aus Kapitel 2 noch einmal an. Hier haben wir 7 Rezensionen zu Filmen, die als Kategorie eine Schulnote von 1-6 erhalten haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Note</th>\n",
       "      <th>Rezension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>r1</th>\n",
       "      <td>1</td>\n",
       "      <td>Der Film war so genial!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>6</td>\n",
       "      <td>Schlechtester Film aller Zeiten!!!!1!!!1!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r3</th>\n",
       "      <td>3</td>\n",
       "      <td>Er war schon okay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r4</th>\n",
       "      <td>1</td>\n",
       "      <td>Ich will den Film für den Rest meines Lebens j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r5</th>\n",
       "      <td>2</td>\n",
       "      <td>Richtig guter Film mit kleineren Schwächen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r6</th>\n",
       "      <td>4</td>\n",
       "      <td>Naja, gibt besseres, aber auch schlechteres, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r7</th>\n",
       "      <td>5</td>\n",
       "      <td>Bis auf die Songauswahl war der Film zum Kotzen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Note                                          Rezension\n",
       "r1     1                            Der Film war so genial!\n",
       "r2     6         Schlechtester Film aller Zeiten!!!!1!!!1!!\n",
       "r3     3                                  Er war schon okay\n",
       "r4     1  Ich will den Film für den Rest meines Lebens j...\n",
       "r5     2         Richtig guter Film mit kleineren Schwächen\n",
       "r6     4  Naja, gibt besseres, aber auch schlechteres, w...\n",
       "r7     5    Bis auf die Songauswahl war der Film zum Kotzen"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "d = {\"r1\": [1, \"Der Film war so genial!\"], \n",
    "     \"r2\": [6, \"Schlechtester Film aller Zeiten!!!!1!!!1!!\"],\n",
    "     \"r3\": [3, \"Er war schon okay\"],\n",
    "     \"r4\": [1, \"Ich will den Film für den Rest meines Lebens jeden Tag gucken :O\"],\n",
    "     \"r5\": [2, \"Richtig guter Film mit kleineren Schwächen\"],\n",
    "     \"r6\": [4, \"Naja, gibt besseres, aber auch schlechteres, wenn auch nicht viel...\"],\n",
    "     \"r7\": [5, \"Bis auf die Songauswahl war der Film zum Kotzen\"]}\n",
    "corpus = pd.DataFrame.from_dict(d, orient=\"index\", columns=[\"Note\", \"Rezension\"])\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir verwenden unser Klassifizierungsverfahren aus dem vorherigen Kapitel und verändern nur die Spaltenbezeichnungen (`corpus[\"Kategorie\"] → corpus[\"Note\"]; corpus[\"Text\"] → corpus[\"Rezension\"]`). Zudem schauen wir uns hier nicht die kodierte Repräsentation des Voraussage des Klassenlabels an, da dies verwirrend ist, wenn die Klassenlabels Zahlen sind (Die Note \"1\" wird hier zur 0, die Note \"2\" wird zur \"1\", die Note \"3\" wird zur \"2\" usw.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Text in numerische Repräsentation transformieren\n",
    "vectorizer = CountVectorizer()\n",
    "vector = vectorizer.fit_transform(corpus[\"Rezension\"])\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "classifier = MultinomialNB()\n",
    "mnb = classifier.fit(vector, corpus[\"Note\"])\n",
    "\n",
    "# Unser Testsatz\n",
    "vector2 = vectorizer.transform([\"Ein sehr knappes Spiel\"])\n",
    "\n",
    "# Die Voraussage\n",
    "prediction = mnb.predict(vector2)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Satz \"Ein sehr knappes Spiel\" wird der Note \"1\" zugeordnet. Diese Zuordnung scheint sehr willkürlich, was daran liegt, dass der Satz im Kontext einer Filmrezension keine wirkliche Bedeutung hat. Versuchen wir es einmal mit Sätzen, die eher einer Filmrezension ähneln."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Rezension 'Ein guter Film' steht für die Note [2].\n",
      "Die Rezension 'Ein schlechter Film' steht für die Note [1].\n",
      "Die Rezension 'Der Film war okay' steht für die Note [1].\n"
     ]
    }
   ],
   "source": [
    "# Unser Testsatz\n",
    "review3 = \"Ein guter Film\"\n",
    "review4 = \"Ein schlechter Film\"\n",
    "review5 = \"Der Film war okay\"\n",
    "vector3 = vectorizer.transform([review3])\n",
    "vector4 = vectorizer.transform([review4])\n",
    "vector5 = vectorizer.transform([review5])\n",
    "\n",
    "# Die Voraussage\n",
    "prediction3 = mnb.predict(vector3)\n",
    "prediction4 = mnb.predict(vector4)\n",
    "prediction5 = mnb.predict(vector5)\n",
    "\n",
    "\n",
    "print(f\"Die Rezension '{review3}' steht für die Note \" \n",
    "      + str(prediction3) + \".\")\n",
    "\n",
    "print(f\"Die Rezension '{review4}' steht für die Note \" \n",
    "      + str(prediction4) + \".\")\n",
    "\n",
    "print(f\"Die Rezension '{review5}' steht für die Note \" \n",
    "      + str(prediction5) + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Ergebnis mag etwas verwundern. Während die Klassifizierung des ersten Satzes \"Ein guter Film\" noch sinnvoll erscheint, sind die anderen beiden Klassifizierungen weniger verständlich. Doch woran liegt das? Eine Antwort könnte folgendes Beispiel liefern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Rezension 'Ein schlechtester Film' steht für die Note [6].\n"
     ]
    }
   ],
   "source": [
    "review6 = \"Ein schlechtester Film\"\n",
    "vector6 = vectorizer.transform([review6])\n",
    "prediction6 = mnb.predict(vector6)\n",
    "\n",
    "print(f\"Die Rezension '{review6}' steht für die Note \" \n",
    "      + str(prediction6) + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Satz ist grammatikalisch nicht korrekt, klassifiziert aber besser als der semantisch ähnliche Satz \"Ein schlechter Film\". Das liegt an der Eigenart des Bag-of-Words Modells: Es teilt Sätze in Wörter auf, die Wortbedeutungen werden jedoch nicht berücksichtigt. Die Wörter \"schlechtester\" und \"schlechter\" sind für das Bag-of-Words-Modell genauso ähnlich zueinander wie \"schlechter\" und \"gut\". Wenn wir für unsere Rezension also nicht genau die Wörter benutzen, die in den Datensätzen verwendet werden, klassifiziert unser Textklassifikationsverfahren nicht richtig.<br>\n",
    "Ein weiteres Problem ist die <b>Häufigkeit von Wörtern</b> (englisch: word frequency). Umso häufiger ein Wort in einem Text vorkommt, umso höher ist sein Einfluss auf die Berechnung bei der Naive Bayes Klassifikation.\n",
    "So lässt sich auch die falsche Klassifikation von \"Der Film war okay\" erklären. Das Wort \"okay\" taucht zwar in der Rezension r3 \"Er war schon okay\" mit der Note \"3\" auf, das Wort \"Film\" taucht jedoch in zwei Rezensionen mit der Note \"1\" auf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Bag-of-Words Modell scheint in der Ausführung, in der wir es kennen, limitiert zu sein. Wörter, die öfters vorkommen, haben einen größeren Einfluss auf die Klassifikation als Wörter, die seltener vorkommen. Selten bieten jedoch häufig auftretende Wörter einen großen Informationsgehalt für die Klassifizierung. Diese Wörter werden auch Stoppwörter genannt. Es gibt zwei Möglichkeiten, den Einfluss der Stoppwörter zu verringern: Entweder werden alle Stoppwörter <u>entfernt</u> oder sie werden <u>skaliert</u>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Exkurs:</b> Stoppwörter <br>\n",
    "    \n",
    "<b>Stoppwörter</b> (englisch: stop words) sind Wörter, die sehr häufig in Texten auftreten, aber einen niedrigen Informationsgehalt besitzen. Übliche Stoppwörter in der deutschen Sprache sind:\n",
    "- bestimmte Artikel (\"der\", \"die\", \"das\")\n",
    "- unbestimmte Artikel (\"eine\", \"einer\", \"ein\", \"eines\")\n",
    "- Konjunktionen (\"und\", \"oder\", \"doch\", \"weil\")\n",
    "- Präpositionen (\"in\", \"an\", \"von\", \"bei\")\n",
    "- Negation (\"nicht\")\n",
    "\n",
    "Zudem gibt es noch die <b>Stoppzeichen</b>, die oft auch unter die Kategorie der Stoppwörter fallen. Beispiele wären der Punkt (\".\"), das Komma (\",\") oder der Strichpunkt (\";\").\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da wir aktuell nur einen sehr kleinen Datensatz behandeln, würde nach eine Entfernung der Stoppwörter dieser noch kleiner werden. Deshalb behandeln wir nun die Methode der <u>Skalierung</u>. Dafür verwenden wir das <b>Tf-Idf-Maß</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Exkurs:</b> Tf-Idf-Maß <br>\n",
    "    \n",
    "Das <b>Tf-Idf-Maß</b> kommt eigentlich aus dem Bereich des <b>Information Retrieval</b> und wird dort zur Beurteilung der Relevanz von Termen in Dokumenten einer Kollektion von Dokumenten eingesetzt.<br>\n",
    "Das Maß besteht aus zwei Kompontenten:<br>\n",
    "- <b>tf</b> (= term frequency, deutsch: Termhäufigkeit)\n",
    "- <b>idf</b> (= inverse document frequency, deutsch: invertierte Dokumenthäufigkeit)\n",
    "\n",
    "Die Termhäufigkeit <b>tf</b> kann in mehreren Variationen vorliegen. Die einfachste Variante wird <b>\"einfache Termhäufigkeit\"</b> genannt. Dabei wird wie beim Bag-of-Words Modell die Häufigkeit der Wörter gezählt. Häufigere Wörter in einem Text sind somit wichtiger. Eine weitere Variante wäre die <b>normierte Termhäufigkeit</b>, bei der die Häufigkeit eines Wortes <i>f</i> durch die maximale Worthäufigkeit (also die Häufigkeit des häufigsten Wortes) geteilt wird. Diese Variationen sollen durch zwei Beispiele genauer erläutert werden.<br>\n",
    "\n",
    "<u>Beispiele</u>:<br>\n",
    "Wir haben die folgenden Sätze vorliegen:<br>\n",
    "$ s_1 $= \"groß groß stark stark\" <br>\n",
    "$ s_2 $= \"groß groß groß groß klein klein klein stark\" <br>\n",
    "Um mit ihnen rechnen zu können, müssen wir sie in Vektoren umwandeln. Dazu müssen wir zuerst einen Vektorraum[<sup>1</sup>](#fn1) bestimmen. Die Dimensionalität dieses Raums wird durch die verschiedenen Wörter unseres Datensätze (hier die Sätze $ s_1 $ und $ s_2 $) bestimmt. Unser Raum ist dreidimensional, da unser Datensatz nur drei verschiedene Wörter enthält: groß, stark, klein. Wir definieren unseren Vektorraum also folgendermaßen: (groß, stark, klein).<br><br>\n",
    "\n",
    "Bei der <b>einfachen Termhäufigkeit</b> würden die Vektoren für die Sätze $ s_1 $ und $ s_2 $ folgendermaßen aussehen:<br>\n",
    "$ s_1 = (2, 0, 2) $<br>\n",
    "$ s_2 = (4, 3, 1) $<br>\n",
    "\n",
    "Die Häufigkeit der Wörter in den Sätzen wird gezählt und dann in den Vektorraum <b>(groß, stark, klein)</b> \"eingesetzt\".<br><br>\n",
    "\n",
    "Bei der <b>normierten Termhäufigkeit</b> würden die Vektoren für die Sätze $ s_1 $ und $ s_2 $ folgendermaßen aussehen:<br>\n",
    "$ s_1 = (1, 0, 1) $<br>\n",
    "$ s_2 = (1, \\frac{3}{4}, \\frac{1}{4}) $<br>\n",
    "\n",
    "Für jeden Satz wird zuerst das häufigste Wort im Satz bestimmt. Im ersten Satz $ s_1 $ kommen die Wörter \"groß\" und \"stark\" jeweils 2 Mal vor. Dies ist der Wert, durch den alle einfachen Termhäufigkeiten geteilt werden. Der Satz $ s_1 $ bei der einfachen Termhäufigkeit wird also zu (1, 0, 1), da $ s_1 = (\\frac{2}{2}, \\frac{0}{2}, \\frac{2}{2}) = (1, 0, 1) $. Genauso funktioniert es beim zweiten Satz. Das häufigste Wort dort ist \"groß\", es kommt 4 Mal vor. Alle einfachen Termhäufigkeiten werden also durch 4 geteilt und man erhält $ (1, \\frac{3}{4}, \\frac{1}{4}) $.<br><br>\n",
    "\n",
    "Die Formel für die invertiere Dokumenthäufigkeit <b>idf</b> ist $ log(\\frac{N}{df_i}) $. $ N $ ist die Anzahl aller Texte oder Dokumente. $ {df_i} $ ist die Dokumenthäufigkeit des Worts $ i $, d.h. die Anzahl der Dokumente, die das Wort $ i $ enthalten. Der Logarithmus wird hier genutzt, um die Auswirkung von sehr häufigen Wörtern zu dämpfen. Dazu schauen wir uns die Formel für die <b>tf-idf-Gewichtung</b> an: $ tf \\cdot idf $. Diese Gewichtung gibt jedem Wort in unseren Datensätzen ein Gewicht. Worte, die sehr häufig in allen Texten vorkommen, erhalten eine sehr niedrige oder gar keine Gewichtung. Wörter, die sehr häufig in einem bestimmten Dokument vorkommen, aber selten in anderen Texten, erhalten ein sehr hohes Gewicht.<br>\n",
    "\n",
    "<u>Beispiel</u>:<br>\n",
    "Wir haben den Satz mit den folgenden Wörtern vorliegen: $ s3 = \"groß, stark, klein, stark, stark\" $. Für jedes Wort dieses Satzes berechnen wir die einfache Termhäufigkeit:<br>\n",
    "$ tf_{groß} = 1$<br>\n",
    "$ tf_{stark} = 3$<br>\n",
    "$ tf_{klein} = 1$<br>\n",
    "Als Vektor im Vektorraum (groß, stark, klein) würde dies $ (1, 3, 1) $  geschrieben werden.<br>\n",
    "\n",
    "Wir nehmen nun an, dass wir 10000 Texte haben. Für jedes Wort haben wir nun die Dokumenthäufigkeit gezählt, also in wievielen Dokumenten das Wort vorkommt. Das Ergebnis ist:<br>\n",
    "$ df_{groß} = 50$<br>\n",
    "$ df_{stark} = 1300$<br>\n",
    "$ df_{klein} = 250$<br>\n",
    "\n",
    "Nun setzen wir unsere Werte in die Formel für die <b>tf-idf-Gewichtung</b> ($ tf \\cdot log(\\frac{N}{df_i}) $) ein (wir verwenden hier den natürlichen Logarithmus):<br>\n",
    "$ tf\\text{-}idf_{groß} = 1 \\cdot log(\\frac{10000}{50}) = 1 \\cdot 5.3 = 5.3 $<br>\n",
    "$ tf\\text{-}idf_{stark} = 3 \\cdot log(\\frac{10000}{1300}) = 3 \\cdot 2.0 = 6.0 $<br>\n",
    "$ tf\\text{-}idf_{klein} = 1 \\cdot log(\\frac{10000}{250}) = 1 \\cdot 3.7 = 3.7 $<br>\n",
    "\n",
    "Angenommen, wir hätten nun ein Wort wie \"der\", welches die Termhäufigkeit 350 und die Dokumenthäufigkeit 10000 besitzen würde. Da es in allen Dokumenten vorkommt, wäre die tf-idf-Gewichtung folgende:<br>\n",
    "$ tf\\text{-}idf_{der} = 350 \\cdot log(\\frac{10000}{10000}) = 350 \\cdot log(1) = 350 \\cdot 0 = 0 $<br>\n",
    "Das Besonderheit des Logarithmus wird hier ausgenutzt, da $log(1)$ immer null ist. Wörter, die in (beinahe) allen Texten vorkommen wie Stoppwörter werden also gedämpft und erhalten eine niedrige oder gar keine Gewichtung.\n",
    "\n",
    "\n",
    "<hr style=\"border: 0.1px solid black;\"/>\n",
    "<span id=\"fn1\" style=\"font-size:8pt; line-height:1\"><sup style=\"font-size:5pt\">1</sup> &nbsp; \n",
    "Ich greife hier auf das Vektorraummodell aus dem Bereich des <b>Information Retrieval</b> zurück. Da es hier nur benutzt wird, um das tf-idf-Maß zu erklären, wird es nicht weiter behandelt. Vertiefende Informationen zum Vektorraummodell bietet z.B. die <a href=\"https://de.wikipedia.org/wiki/Vektorraum-Retrieval\">Wikipedia</a>-Seite des Vektorraummodells.</span>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Der Tf-idf Vectorizer <a class=\"anchor\" id=\"4-3\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1. Anwendung auf die Film-Rezensionen <a class=\"anchor\" id=\"4-3-1\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anstatt des Bag-of-Words Modells verwenden wir nun das Tf-idf-Maß. Dazu tauschen wir den `CountVectorizer` durch den `TfidfVectorizer` aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "# Text in numerische Repräsentation transformieren\n",
    "vectorizer = TfidfVectorizer()\n",
    "vector = vectorizer.fit_transform(corpus[\"Rezension\"])\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "classifier = MultinomialNB()\n",
    "mnb = classifier.fit(vector, corpus[\"Note\"])\n",
    "\n",
    "# Unser Testsatz\n",
    "vector2 = vectorizer.transform([\"Ein sehr knappes Spiel\"])\n",
    "\n",
    "# Die Voraussage\n",
    "prediction = mnb.predict(vector2)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternativ könnten wir mit Scikit learn auch erst den `CountVectorizer` nutzen und diesen dann mit dem `TfIdfTransformer` transformieren. Dies ist m. E. aber umständlicher als den `TfidfVectorizer` direkt zu nutzen und produziert nur mehr Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "# Text in numerische Repräsentation transformieren\n",
    "vectorizer = CountVectorizer()\n",
    "vector = vectorizer.fit_transform(corpus[\"Rezension\"])\n",
    "transformer = TfidfTransformer()\n",
    "vector = transformer.fit_transform(vector)\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "classifier = MultinomialNB()\n",
    "mnb = classifier.fit(vector, corpus[\"Note\"])\n",
    "\n",
    "# Unser Testsatz\n",
    "vector2 = vectorizer.transform([\"Ein sehr knappes Spiel\"])\n",
    "\n",
    "# Die Voraussage\n",
    "prediction = mnb.predict(vector2)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Rezension 'Ein guter Film' steht für die Note [1].\n",
      "Die Rezension 'Ein schlechter Film' steht für die Note [1].\n",
      "Die Rezension 'Der Film war okay' steht für die Note [1].\n"
     ]
    }
   ],
   "source": [
    "# Unser Testsatz\n",
    "review3 = \"Ein guter Film\"\n",
    "review4 = \"Ein schlechter Film\"\n",
    "review5 = \"Der Film war okay\"\n",
    "vector3 = vectorizer.transform([review3])\n",
    "vector4 = vectorizer.transform([review4])\n",
    "vector5 = vectorizer.transform([review5])\n",
    "\n",
    "# Die Voraussage\n",
    "prediction3 = mnb.predict(vector3)\n",
    "prediction4 = mnb.predict(vector4)\n",
    "prediction5 = mnb.predict(vector5)\n",
    "\n",
    "\n",
    "print(f\"Die Rezension '{review3}' steht für die Note \" \n",
    "      + str(prediction3) + \".\")\n",
    "\n",
    "print(f\"Die Rezension '{review4}' steht für die Note \" \n",
    "      + str(prediction4) + \".\")\n",
    "\n",
    "print(f\"Die Rezension '{review5}' steht für die Note \" \n",
    "      + str(prediction5) + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Ergebnisse sind hier tatsächlich ungenauer als beim `CountVectorizer`. Das könnte daran liegen, dass unser Datensatz sehr klein ist und nur sehr wenige Wörter beinhaltet. Um einen Unterschied zwischen den Vectorizern zu erkennen, brauchen wir einen größeren Datensatz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2. Anwendung auf das Wikipedia-Korpus <a class=\"anchor\" id=\"4-3-2\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>length</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3470</td>\n",
       "      <td>Album nach Typ</td>\n",
       "      <td>1050</td>\n",
       "      <td>All the Best ! ( englisch Alles Gute ! ) ist d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3611</td>\n",
       "      <td>Album nach Typ</td>\n",
       "      <td>525</td>\n",
       "      <td>Let It Roll : Songs by George Harrison ist das...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3612</td>\n",
       "      <td>Album nach Typ</td>\n",
       "      <td>251</td>\n",
       "      <td>Lieder wie Orkane ist das dritte offizielle Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3613</td>\n",
       "      <td>Album nach Typ</td>\n",
       "      <td>756</td>\n",
       "      <td>Long Stick Goes Boom : The Anthology ist eine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3614</td>\n",
       "      <td>Album nach Typ</td>\n",
       "      <td>260</td>\n",
       "      <td>Los Grandes Éxitos en Español ( spanisch für D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id        category  length  \\\n",
       "0  3470  Album nach Typ    1050   \n",
       "1  3611  Album nach Typ     525   \n",
       "2  3612  Album nach Typ     251   \n",
       "3  3613  Album nach Typ     756   \n",
       "4  3614  Album nach Typ     260   \n",
       "\n",
       "                                                text  \n",
       "0  All the Best ! ( englisch Alles Gute ! ) ist d...  \n",
       "1  Let It Roll : Songs by George Harrison ist das...  \n",
       "2  Lieder wie Orkane ist das dritte offizielle Be...  \n",
       "3  Long Stick Goes Boom : The Anthology ist eine ...  \n",
       "4  Los Grandes Éxitos en Español ( spanisch für D...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "corpus = pd.read_csv(\"tutorialdata/corpora/wikicorpus_v2.csv\", index_col=0)\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir verwenden unser Klassifizierungsverfahren aus dem vorherigen Abschnitt und verändern nur die Spaltenbezeichnungen (`corpus[\"Note\"] → corpus[\"category\"]; corpus[\"Rezension\"] → corpus[\"text\"]`). Als Beispielsatz nehmen wir erneut den Satz \"Ein sehr knappes Spiel\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Computerspiel nach Plattform']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "# Text in numerische Repräsentation transformieren\n",
    "vectorizer = TfidfVectorizer()\n",
    "vector = vectorizer.fit_transform(corpus[\"text\"])\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "classifier = MultinomialNB()\n",
    "mnb = classifier.fit(vector, corpus[\"category\"])\n",
    "\n",
    "# Unser Testsatz\n",
    "vector2 = vectorizer.transform([\"Ein sehr knappes Spiel\"])\n",
    "\n",
    "# Die Voraussage\n",
    "prediction = mnb.predict(vector2)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer im Vergleich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Computerspiel nach Plattform']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "# Text in numerische Repräsentation transformieren\n",
    "vectorizer = CountVectorizer()\n",
    "vector = vectorizer.fit_transform(corpus[\"text\"])\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "classifier = MultinomialNB()\n",
    "mnb = classifier.fit(vector, corpus[\"category\"])\n",
    "\n",
    "# Unser Testsatz\n",
    "vector2 = vectorizer.transform([\"Ein sehr knappes Spiel\"])\n",
    "\n",
    "# Die Voraussage\n",
    "prediction = mnb.predict(vector2)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Satz \"Ein sehr knappes Spiel\" wurde der Kategorie \"Computerspiel nach Plattform\" bei beiden Vectorizern zugeordnet. Um bewerten zu können, ob diese Einteilung sinnvoll erscheint, schauen wir uns einmal die möglichen Kategorien an."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bewertung der Einteilung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Album nach Typ' 'Alternative-Rock-Band' 'Burg in Deutschland'\n",
      " 'Chemikaliengruppe' 'Computerspiel nach Plattform'\n",
      " 'Deutsche Vereine nach Bundesland' 'Einzelsprache'\n",
      " 'Fernsehserie nach Staat' 'Film nach Staat' 'Frauenfußball'\n",
      " 'Gemälde nach Jahrhundert' 'Herrscher nach Titel' 'Internet'\n",
      " 'Kostümkunde' 'Krankheit' 'Kreditgeschäft' 'Krieg nach Typ'\n",
      " 'Kunst (Deutschland)' 'Literatur (Romantik)' 'Literaturwissenschaft'\n",
      " 'Logik' 'Millionenstadt' 'Oper' 'PKW-Modell' 'Planung und Organisation'\n",
      " 'See in Deutschland' 'Soziologie' 'Wald' 'Western' 'Wirtschaft']\n"
     ]
    }
   ],
   "source": [
    "print(corpus.category.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viele der Kategorien scheinen nicht zum Satz \"Ein sehr knappes Spiel\" zu passen. In unserem ursprünglichen Beispiel wurde der Satz der Kategorie \"Sport\" zugewiesen. Eine verwandte Kategorie befindet sich auch in diesem Datensatz: \"Frauenfußball\". Anstatt der Kategorie \"Frauenfußball\" wurde der Satz jedoch der Kategorie \"Computerspiel nach Plattform\" zugeordnet. Um eine Erklärung dafür zu finden, schauen wir uns die Texte der beiden Kategorien genauer an."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kategorie: Frauenfußball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>length</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>Frauenfußball</td>\n",
       "      <td>2236</td>\n",
       "      <td>Frauenfußball ist auch in Deutschland eine der...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>Frauenfußball</td>\n",
       "      <td>848</td>\n",
       "      <td>Der Artikel beinhaltet eine ausführliche Darst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1802</th>\n",
       "      <td>Frauenfußball</td>\n",
       "      <td>355</td>\n",
       "      <td>Die Newcastle United Jets Women sind ein austr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1803</th>\n",
       "      <td>Frauenfußball</td>\n",
       "      <td>168</td>\n",
       "      <td>Der Verein für Rasensport Niederfell 1949 e.V ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804</th>\n",
       "      <td>Frauenfußball</td>\n",
       "      <td>1182</td>\n",
       "      <td>Der Artikel beinhaltet eine ausführliche Darst...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           category  length                                               text\n",
       "1800  Frauenfußball    2236  Frauenfußball ist auch in Deutschland eine der...\n",
       "1801  Frauenfußball     848  Der Artikel beinhaltet eine ausführliche Darst...\n",
       "1802  Frauenfußball     355  Die Newcastle United Jets Women sind ein austr...\n",
       "1803  Frauenfußball     168  Der Verein für Rasensport Niederfell 1949 e.V ...\n",
       "1804  Frauenfußball    1182  Der Artikel beinhaltet eine ausführliche Darst..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frauenfußball = corpus.loc[corpus[\"category\"] == \"Frauenfußball\"]\n",
    "frauenfußball = frauenfußball.drop([\"id\"], axis=1)\n",
    "frauenfußball.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "frauenfußball_text = frauenfußball.text.to_string()\n",
    "frauenfußball_counter = dict(Counter(word_tokenize(frauenfußball_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Wort 'Ein' kommt kein einziges Mal vor.\n",
      "Das Wort 'sehr' kommt kein einziges Mal vor.\n",
      "Das Wort 'knappes' kommt kein einziges Mal vor.\n",
      "Das Wort 'Spiel' kommt kein einziges Mal vor.\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"Ein\", \"sehr\", \"knappes\", \"Spiel\"]\n",
    "for word in sentence:\n",
    "    \n",
    "    if word in frauenfußball_counter:\n",
    "        print(f\"Das Wort '{word}' kommt \" + str(frauenfußball_counter[word]) + \" Mal vor.\")\n",
    "    else:\n",
    "        print(f\"Das Wort '{word}' kommt kein einziges Mal vor.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keines der Wörter des Satzes \"Ein sehr knappes Spiel\" kommt in den Texten der Kategorie \"Frauenfußball\" vor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kategorie: Computerspiel nach Plattform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>length</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>Computerspiel nach Plattform</td>\n",
       "      <td>353</td>\n",
       "      <td>Ballyhoo ist ein Computerspiel der US-amerikan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>Computerspiel nach Plattform</td>\n",
       "      <td>379</td>\n",
       "      <td>Balance of Power ist ein Computer-Strategiespi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>Computerspiel nach Plattform</td>\n",
       "      <td>415</td>\n",
       "      <td>Ballblazer ist ein Zweispieler-Computer-Sports...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>Computerspiel nach Plattform</td>\n",
       "      <td>417</td>\n",
       "      <td>Barbarian : The Ultimate Warrior und Barbarian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>Computerspiel nach Plattform</td>\n",
       "      <td>359</td>\n",
       "      <td>Beyond Zork ( kompletter Titel Beyond Zork : T...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         category  length  \\\n",
       "800  Computerspiel nach Plattform     353   \n",
       "801  Computerspiel nach Plattform     379   \n",
       "802  Computerspiel nach Plattform     415   \n",
       "803  Computerspiel nach Plattform     417   \n",
       "804  Computerspiel nach Plattform     359   \n",
       "\n",
       "                                                  text  \n",
       "800  Ballyhoo ist ein Computerspiel der US-amerikan...  \n",
       "801  Balance of Power ist ein Computer-Strategiespi...  \n",
       "802  Ballblazer ist ein Zweispieler-Computer-Sports...  \n",
       "803  Barbarian : The Ultimate Warrior und Barbarian...  \n",
       "804  Beyond Zork ( kompletter Titel Beyond Zork : T...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computerspiel = corpus.loc[corpus[\"category\"] == \"Computerspiel nach Plattform\"]\n",
    "computerspiel = computerspiel.drop([\"id\"], axis=1)\n",
    "computerspiel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "computerspiel_text = computerspiel.text.to_string()\n",
    "computerspiel_counter = dict(Counter(word_tokenize(computerspiel_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Wort 'Ein' kommt kein einziges Mal vor.\n",
      "Das Wort 'sehr' kommt kein einziges Mal vor.\n",
      "Das Wort 'knappes' kommt kein einziges Mal vor.\n",
      "Das Wort 'Spiel' kommt 1 Mal vor.\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"Ein\", \"sehr\", \"knappes\", \"Spiel\"]\n",
    "for word in sentence:\n",
    "    \n",
    "    if word in computerspiel_counter:\n",
    "        print(f\"Das Wort '{word}' kommt \" + str(computerspiel_counter[word]) + \" Mal vor.\")\n",
    "    else:\n",
    "        print(f\"Das Wort '{word}' kommt kein einziges Mal vor.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bis auf das Wort \"Spiel\" kommt keines der anderen Wörter in den Texten der Kategorie \"Computerspiel nach Plattform\" vor. Da jedoch keines der Wörter in der Kategorie \"Frauenfußball\" vorkam, wurde für den Satz \"Ein sehr knappes Spiel\" die Kategorie \"Computerspiel nach Plattform\" gewählt.<br>\n",
    "\n",
    "Hierbei wurden nun einige Dinge nicht beachtet:\n",
    "- Art der Datensätze (Nachrichten Artikel, Lexikon Artikel)\n",
    "- Groß- und Kleinschreibung (\"ein\", \"Ein\")\n",
    "- Kompositionen (\"Spiel\", \"Spielereihe\")\n",
    "- Rechtschreibfehler (\"Spiel\", \"Spieel\")\n",
    "- Angefügte Zeichen (\"Spiel\", \"Spiel-\")<br>\n",
    "\n",
    "Eine genaue Erklärung, warum das Klassifizierungsverfahren eine bestimmte Kategorie für einen Satz oder einen Text genommen wurde, lässt sich nicht immer bestimmen. Vor allem bei größeren Datensätzen ist eine Nachverfolgung müheselig oder kompliziert. Um ein Klassifikationsverfahren und seine Klassifizierung bewerten zu können, brauchen wir <b>Evaluations</b> Methoden."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
